{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HpuCaAvTa0NZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense\n",
        "import tensorflow as tf\n",
        "\n",
        "import pickle\n",
        "import joblib\n",
        "import json\n",
        "\n",
        "\n",
        "# Read the CSV file and preprocess the data\n",
        "file_path = \"stock_data_df.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Cr7xicBK-Yl"
      },
      "source": [
        "# New Section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IZ1647Ofwiv3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DCsvqq9FxByg"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrZYhpC4xPuS"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R41li2KExbyE"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30yZAx3DCbM2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qglJRwPRCbM3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIfjHSPJCbM3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95Qw5OX8CbM4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RWnT239fCbM4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zeu8Gm97CbM4"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYUnplMaCbM4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyC3BjrgCbM5",
        "outputId": "125565a2-0680-469e-c19c-43a843a2f1b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/60\n",
            "23/23 [==============================] - 3s 12ms/step - loss: 0.9582\n",
            "Epoch 2/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8139\n",
            "Epoch 3/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8168\n",
            "Epoch 4/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8099\n",
            "Epoch 5/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8213\n",
            "Epoch 6/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8208\n",
            "Epoch 7/60\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.8074\n",
            "Epoch 8/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8176\n",
            "Epoch 9/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8159\n",
            "Epoch 10/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8146\n",
            "Epoch 11/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8350\n",
            "Epoch 12/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8416\n",
            "Epoch 13/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8368\n",
            "Epoch 14/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8152\n",
            "Epoch 15/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8210\n",
            "Epoch 16/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8123\n",
            "Epoch 17/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8173\n",
            "Epoch 18/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8175\n",
            "Epoch 19/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8125\n",
            "Epoch 20/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8277\n",
            "Epoch 21/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8137\n",
            "Epoch 22/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8202\n",
            "Epoch 23/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8203\n",
            "Epoch 24/60\n",
            "23/23 [==============================] - 0s 19ms/step - loss: 0.8176\n",
            "Epoch 25/60\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.8267\n",
            "Epoch 26/60\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.8160\n",
            "Epoch 27/60\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.8148\n",
            "Epoch 28/60\n",
            "23/23 [==============================] - 0s 17ms/step - loss: 0.8185\n",
            "Epoch 29/60\n",
            "23/23 [==============================] - 0s 16ms/step - loss: 0.8136\n",
            "Epoch 30/60\n",
            "23/23 [==============================] - 0s 15ms/step - loss: 0.8179\n",
            "Epoch 31/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8179\n",
            "Epoch 32/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8124\n",
            "Epoch 33/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8098\n",
            "Epoch 34/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8295\n",
            "Epoch 35/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8201\n",
            "Epoch 36/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8182\n",
            "Epoch 37/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8148\n",
            "Epoch 38/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8202\n",
            "Epoch 39/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8123\n",
            "Epoch 40/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8128\n",
            "Epoch 41/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8105\n",
            "Epoch 42/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8155\n",
            "Epoch 43/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8111\n",
            "Epoch 44/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8124\n",
            "Epoch 45/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8116\n",
            "Epoch 46/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8184\n",
            "Epoch 47/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8206\n",
            "Epoch 48/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8223\n",
            "Epoch 49/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.8201\n",
            "Epoch 50/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8222\n",
            "Epoch 51/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8176\n",
            "Epoch 52/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8256\n",
            "Epoch 53/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.9042\n",
            "Epoch 54/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8894\n",
            "Epoch 55/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8126\n",
            "Epoch 56/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8133\n",
            "Epoch 57/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8123\n",
            "Epoch 58/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.8111\n",
            "Epoch 59/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8127\n",
            "Epoch 60/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.8169\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7c45bcfa4490>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AB3o-iKrxi9x",
        "outputId": "9f8b9fc7-c304-4630-a8fc-9de66811be21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 6ms/step - loss: 1.4801\n",
            "Mean Squared Error for WH: 1.4800537824630737\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kotC6j1wCbM5",
        "outputId": "790c7a6c-69aa-4652-85e2-d784c72306b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amitk\\anaconda3.1\\lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "#joblib.dump(model, 'LSTM_model.pkl')\n",
        "model.save('LSTM_model.h5')\n",
        "model.save('my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8sNIFZd21Ta",
        "outputId": "294c6071-d0b1-4d33-dda5-a9d6c35fda8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 1s 6ms/step\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnECIC4c2RCv",
        "outputId": "94f6fc27-0b76-41d5-cfef-078309f30a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 1.4800538376323609\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIit70bW2AG9",
        "outputId": "fc46d4b6-de37-44b0-9bdb-dd454d39ab60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6/6 [==============================] - 0s 5ms/step\n"
          ]
        }
      ],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MD027ZlF2FfJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee0f899b-5fd5-4ef7-9255-a834ef8b7580"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(181, 2)\n",
            "(180, 5)\n",
            "[[  3820.31192352 420252.13406235]\n",
            " [  3691.638654   420252.13406235]\n",
            " [  2944.46910045 420252.13406235]\n",
            " [  4664.32000225 420252.13406235]\n",
            " [  2803.64748764 420252.13406235]\n",
            " [  4632.57941404 420252.13406235]\n",
            " [  3793.2169292  420252.13406235]\n",
            " [  2837.72814402 420252.13406235]\n",
            " [  4554.8521228  420252.13406235]\n",
            " [  3661.38612979 420252.13406235]\n",
            " [  3817.79224429 420252.13406235]\n",
            " [  2721.34086901 420252.13406235]\n",
            " [  4690.87641756 420252.13406235]\n",
            " [  3697.81273783 420252.13406235]\n",
            " [  3556.2898215  420252.13406235]\n",
            " [  3497.16204996 420252.13406235]\n",
            " [  3241.54254617 420252.13406235]\n",
            " [  4674.68374362 420252.13406235]\n",
            " [  3282.05416633 420252.13406235]\n",
            " [  4701.88901197 420252.13406235]\n",
            " [  3407.58013904 420252.13406235]\n",
            " [  3262.76306444 420252.13406235]\n",
            " [  4637.11422214 420252.13406235]\n",
            " [  3453.32761595 420252.13406235]\n",
            " [  3437.19609164 420252.13406235]\n",
            " [  3274.33726479 420252.13406235]\n",
            " [  4631.28528956 420252.13406235]\n",
            " [  3516.30040488 420252.13406235]\n",
            " [  3149.22987373 420252.13406235]\n",
            " [  3285.26926424 420252.13406235]\n",
            " [  3523.70951057 420252.28860229]\n",
            " [  3135.36703352 420252.28860229]\n",
            " [  3565.07415772 420252.13406235]\n",
            " [  3167.50317643 420252.13406235]\n",
            " [  4595.01193958 420252.13406235]\n",
            " [  3566.30866717 420252.13406235]\n",
            " [  3174.02676107 420252.13406235]\n",
            " [  4490.07733297 420252.13406235]\n",
            " [  3145.09087653 420252.13406235]\n",
            " [  4436.31513755 420252.13406235]\n",
            " [  3214.13284766 420252.13406235]\n",
            " [  3179.81373314 420252.13406235]\n",
            " [  4455.74644877 420252.13406235]\n",
            " [  3796.59424121 420252.13406235]\n",
            " [  3530.45517855 420252.13406235]\n",
            " [  4620.92052469 420252.13406235]\n",
            " [  3573.10013416 420252.13406235]\n",
            " [  3217.28244703 420252.13406235]\n",
            " [  3254.40345091 420252.13406235]\n",
            " [  4502.38456361 420252.13406235]\n",
            " [  3524.32689307 420252.13406235]\n",
            " [  3217.28244703 420252.13406235]\n",
            " [  3258.26177323 420252.13406235]\n",
            " [  3480.4919473  420252.13406235]\n",
            " [  3220.43332478 420252.13406235]\n",
            " [  3412.58009575 420252.13406235]\n",
            " [  3163.09220285 420252.13406235]\n",
            " [  4511.45264523 420252.13406235]\n",
            " [  3453.32761595 420252.13406235]\n",
            " [  3357.2868523  420252.13406235]\n",
            " [  4632.57941404 420252.13406235]\n",
            " [  3390.72402805 420252.13406235]\n",
            " [  4732.3329176  420252.28860229]\n",
            " [  3177.58573117 420252.28860229]\n",
            " [  3359.85949376 420252.13406235]\n",
            " [  4684.39965477 420252.13406235]\n",
            " [  3495.92677322 420252.13406235]\n",
            " [  3142.29819741 420252.13406235]\n",
            " [  3419.01668884 420252.13406235]\n",
            " [  4784.15213483 420252.13406235]\n",
            " [  3516.91855534 420252.13406235]\n",
            " [  3192.70789924 420252.13406235]\n",
            " [  3523.82848377 420252.13406235]\n",
            " [  3506.4222803  420252.13406235]\n",
            " [  3245.00812812 420252.13406235]\n",
            " [  3773.13318265 420252.13406235]\n",
            " [  2849.94480083 420252.13406235]\n",
            " [  4564.56803395 420252.13406235]\n",
            " [  3534.20476211 420252.13406235]\n",
            " [  3258.24104885 420252.13406235]\n",
            " [  4741.40048747 420252.13406235]\n",
            " [  3524.32689307 420252.13406235]\n",
            " [  3387.50867392 420252.13406235]\n",
            " [  4729.74210987 420252.13406235]\n",
            " [  2998.48305895 420252.13406235]\n",
            " [  4444.73590138 420252.13406235]\n",
            " [  3393.0876349  420252.13406235]\n",
            " [  3004.91274369 420252.13406235]\n",
            " [  4598.89789464 420252.13406235]\n",
            " [  3845.36696987 420252.13406235]\n",
            " [  3440.34748181 420252.13406235]\n",
            " [  3093.64904773 420252.13406235]\n",
            " [  4626.10367475 420252.13406235]\n",
            " [  3781.77679779 420252.13406235]\n",
            " [  3398.75870363 420252.28860229]\n",
            " [  3163.09501716 420252.28860229]\n",
            " [  3630.51724593 420252.13406235]\n",
            " [  3480.67488511 420252.13406235]\n",
            " [  3531.73548701 420252.13406235]\n",
            " [  3455.46939434 420252.13406235]\n",
            " [  4607.31865847 420252.13406235]\n",
            " [  3399.08313049 420252.13406235]\n",
            " [  4731.6850874  420252.13406235]\n",
            " [  3415.66700956 420252.13406235]\n",
            " [  3374.81382044 420252.13406235]\n",
            " [  3407.44223227 420252.13406235]\n",
            " [  4648.77413501 420252.13406235]\n",
            " [  3398.37977928 420252.13406235]\n",
            " [  3396.23825711 420252.13406235]\n",
            " [  3407.44223227 420252.13406235]\n",
            " [  3458.26693412 420252.13406235]\n",
            " [  3417.14560711 420252.13406235]\n",
            " [  3510.36094204 420252.13406235]\n",
            " [  3424.11438194 420252.13406235]\n",
            " [  4710.95709359 420252.13406235]\n",
            " [  3535.78851615 420252.13406235]\n",
            " [  3549.12198893 420252.13406235]\n",
            " [  4766.19814211 420252.13406235]\n",
            " [  3536.88614301 420252.13406235]\n",
            " [  4772.04652002 420252.13406235]\n",
            " [  3394.33749564 420252.13406235]\n",
            " [  3538.1738709  420252.13406235]\n",
            " [  4818.84019549 420252.13406235]\n",
            " [  3456.4058306  420252.13406235]\n",
            " [  3379.13217384 420252.13406235]\n",
            " [  3491.80619717 420252.13406235]\n",
            " [  4742.15219512 420252.28860229]\n",
            " [  3508.50035028 420252.28860229]\n",
            " [  3371.53040834 420252.13406235]\n",
            " [  3436.4226361  420252.13406235]\n",
            " [  3509.74048902 420252.13406235]\n",
            " [  3385.46821488 420252.13406235]\n",
            " [  3442.80856951 420252.13406235]\n",
            " [  4679.86535842 420252.13406235]\n",
            " [  3530.20622978 420252.13406235]\n",
            " [  3392.43750213 420252.13406235]\n",
            " [  4666.11458305 420252.13406235]\n",
            " [  3439.66152891 420252.13406235]\n",
            " [  3240.00407804 420252.13406235]\n",
            " [  4690.16053005 420252.13406235]\n",
            " [  3176.24759915 420252.13406235]\n",
            " [  4776.59565658 420252.13406235]\n",
            " [  3381.03319085 420252.13406235]\n",
            " [  3126.65996552 420252.13406235]\n",
            " [  4961.81612365 420252.13406235]\n",
            " [  3442.14231815 420252.13406235]\n",
            " [  3291.068006   420252.13406235]\n",
            " [  3052.60012761 420252.13406235]\n",
            " [  5073.59848555 420252.13406235]\n",
            " [  3448.34403471 420252.13406235]\n",
            " [  3319.57814501 420252.13406235]\n",
            " [  3194.27860522 420252.13406235]\n",
            " [  3499.19790389 420252.13406235]\n",
            " [  3277.76370121 420252.13406235]\n",
            " [  3507.88066522 420252.13406235]\n",
            " [  3268.89365248 420252.13406235]\n",
            " [  5135.33881138 420252.13406235]\n",
            " [  3466.94841574 420252.13406235]\n",
            " [  3296.03086353 420252.28860229]\n",
            " [  5157.43512871 420252.28860229]\n",
            " [  3276.71084985 420252.13406235]\n",
            " [  5106.7437285  420252.13406235]\n",
            " [  3260.76149605 420252.13406235]\n",
            " [  3372.07615124 420252.13406235]\n",
            " [  4633.22775533 420252.13406235]\n",
            " [  3298.67156297 420252.13406235]\n",
            " [  3272.84740998 420252.13406235]\n",
            " [  5037.20488267 420252.13406235]\n",
            " [  3521.85685    420252.13406235]\n",
            " [  3285.9664744  420252.13406235]\n",
            " [  3433.16327444 420252.13406235]\n",
            " [  3437.89304625 420252.13406235]\n",
            " [  3350.86919279 420252.13406235]\n",
            " [  3378.62353012 420252.13406235]\n",
            " [  3306.13030416 420252.13406235]\n",
            " [  3432.95372808 420252.13406235]\n",
            " [  4613.79644411 420252.13406235]\n",
            " [  3377.86363507 420252.13406235]\n",
            " [  4589.82981303 420252.44314224]\n",
            " [  3377.96469819 420252.44314224]\n",
            " [  3389.43783542 420252.44314224]]\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "6zsdJ8w5uckC",
        "outputId": "7e8f549a-0185-4697-f860-ece8c097b46a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-77b9f162d1e6>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#df = df.iloc[:181]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#test_data.loc[:, 'Predicted_Returns'] = unscaled_returns[:, -1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Returns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munscaled_returns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3978\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3979\u001b[0m             \u001b[0;31m# set column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3980\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3982\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_setitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4172\u001b[0m         \u001b[0mensure\u001b[0m \u001b[0mhomogeneity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m         \"\"\"\n\u001b[0;32m-> 4174\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4176\u001b[0m         if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4914\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4915\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4916\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    569\u001b[0m     \"\"\"\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    572\u001b[0m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Length of values (181) does not match length of index (180)"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "5uR4cBM2MGiv",
        "outputId": "e74201b0-45e4-4969-dba6-9812318d5002"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Extract the predicted returns and actual returns from the DataFrame\n",
        "predicted_returns = test_data[\"Predicted_Returns\"]\n",
        "actual_returns = test_data[\"Returns\"]\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.scatter(predicted_returns, actual_returns, alpha=0.5)\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"Predicted Returns\")\n",
        "plt.ylabel(\"Actual Returns\")\n",
        "plt.title(\"Predicted Returns vs. Actual Returns (Scatter Plot)\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xxeROAN2-6k"
      },
      "outputs": [],
      "source": [
        "# using model to predict returns using new data\n",
        "# read data from csv\n",
        "new_data_df = pd.read_csv('C:\\\\Users\\\\amitk\\\\OneDrive\\\\Desktop\\\\Project 4 files\\\\csv files from Mako\\\\2022-23.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Lh7r13nc3e-E",
        "outputId": "b648ef62-c071-46c9-b408-cc78ab5c2532"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>Close</th>\n",
              "      <th>volume</th>\n",
              "      <th>Returns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>3191</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>AMZN</td>\n",
              "      <td>161.485992</td>\n",
              "      <td>87798000.0</td>\n",
              "      <td>3.355245</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>359</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>UAL</td>\n",
              "      <td>46.580002</td>\n",
              "      <td>9204500.0</td>\n",
              "      <td>-0.711554</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>HLT</td>\n",
              "      <td>147.796417</td>\n",
              "      <td>2652300.0</td>\n",
              "      <td>2.172959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>3545</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>WMT</td>\n",
              "      <td>141.254273</td>\n",
              "      <td>7577200.0</td>\n",
              "      <td>-0.044265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>1421</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>MAR</td>\n",
              "      <td>160.183914</td>\n",
              "      <td>2228400.0</td>\n",
              "      <td>0.134011</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1234</th>\n",
              "      <td>1234</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>DAL</td>\n",
              "      <td>32.660000</td>\n",
              "      <td>10958900.0</td>\n",
              "      <td>-0.002228</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3004</th>\n",
              "      <td>3004</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>RUTH</td>\n",
              "      <td>18.221096</td>\n",
              "      <td>274858.0</td>\n",
              "      <td>-0.442097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2650</th>\n",
              "      <td>2650</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>WH</td>\n",
              "      <td>67.461578</td>\n",
              "      <td>678600.0</td>\n",
              "      <td>2.702389</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1588</th>\n",
              "      <td>1588</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>MAR</td>\n",
              "      <td>161.076309</td>\n",
              "      <td>1392400.0</td>\n",
              "      <td>1.387675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2296</th>\n",
              "      <td>2296</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>RCL</td>\n",
              "      <td>47.240002</td>\n",
              "      <td>7783100.0</td>\n",
              "      <td>-0.706723</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3893 rows  6 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0       Date ticker       Close      volume   Returns\n",
              "3191        3191  1/10/2022   AMZN  161.485992  87798000.0  3.355245\n",
              "359          359  1/10/2022    UAL   46.580002   9204500.0 -0.711554\n",
              "5              5  1/10/2022    HLT  147.796417   2652300.0  2.172959\n",
              "3545        3545  1/10/2022    WMT  141.254273   7577200.0 -0.044265\n",
              "1421        1421  1/10/2022    MAR  160.183914   2228400.0  0.134011\n",
              "...          ...        ...    ...         ...         ...       ...\n",
              "1234        1234   9/9/2022    DAL   32.660000  10958900.0 -0.002228\n",
              "3004        3004   9/9/2022   RUTH   18.221096    274858.0 -0.442097\n",
              "2650        2650   9/9/2022     WH   67.461578    678600.0  2.702389\n",
              "1588        1588   9/9/2022    MAR  161.076309   1392400.0  1.387675\n",
              "2296        2296   9/9/2022    RCL   47.240002   7783100.0 -0.706723\n",
              "\n",
              "[3893 rows x 6 columns]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Rename the 'price' column to 'Close'\n",
        "new_data_df = new_data_df.rename(columns={\"price\": \"Close\"})\n",
        "\n",
        "# Drop rows with missing values (NaNs)\n",
        "new_data_df = new_data_df.dropna()\n",
        "\n",
        "# Sort the DataFrame by 'Date' (if it's not already sorted)\n",
        "new_data_df = new_data_df.sort_values('Date')\n",
        "\n",
        "# Calculate returns using the 'shift' function\n",
        "new_data_df['PrevClose'] = new_data_df['Close'].shift(1)\n",
        "new_data_df['Returns'] = (new_data_df['Close'] - new_data_df['PrevClose']) / new_data_df['PrevClose']\n",
        "\n",
        "# Drop the temporary 'PrevClose' column\n",
        "new_data_df = new_data_df.drop('PrevClose', axis=1)\n",
        "new_data_df = new_data_df.dropna()\n",
        "new_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0wkawiq31i6"
      },
      "outputs": [],
      "source": [
        "#Scale the data\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "columns_to_scale = ['Close', 'volume', 'Returns']\n",
        "new_data_scaled = scaler.fit_transform(new_data_df[columns_to_scale])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MIVjeuk36BhA",
        "outputId": "3718ebc2-2b41-453f-d44c-34d3c1a2aa37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_new_with_sequences: (3884, 10, 3)\n"
          ]
        }
      ],
      "source": [
        "# Define the number of past time steps to consider for each sequence\n",
        "timesteps = 10\n",
        "\n",
        "# Create sequences with timesteps\n",
        "X_new_sequences = []\n",
        "for i in range(len(new_data_scaled) - timesteps + 1):\n",
        "    X_new_sequences.append(new_data_scaled[i:i + timesteps])\n",
        "\n",
        "# Convert X_new_sequences to a numpy array\n",
        "X_new_with_sequences = np.array(X_new_sequences)\n",
        "print(\"Shape of X_new_with_sequences:\", X_new_with_sequences.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLnusPQY6Y8C"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJnq9ezYNOky",
        "outputId": "df481331-32dc-44ef-d36e-6ed8c10e5c2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "122/122 [==============================] - 1s 9ms/step\n",
            "Shape of X_new_with_sequences: (3884, 10, 3)\n",
            "122/122 [==============================] - 1s 9ms/step\n",
            "Shape of predicted_returns_scaled: (3884, 1)\n",
            "Shape of y_reshaped: (3884, 1)\n",
            "Mean Squared Error: 8.336908451873791\n"
          ]
        }
      ],
      "source": [
        "# declare variable for actual values\n",
        "y_actual = new_data_df[\"Returns\"]\n",
        "\n",
        "# Make predictions using trained LSTM model\n",
        "predicted_returns_scaled = model.predict(X_new_with_sequences)\n",
        "\n",
        "# Reshape y_actual to match the shape of predicted_returns_scaled\n",
        "y_reshaped = y_actual[-len(predicted_returns_scaled):].values.reshape(-1, 1)\n",
        "\n",
        "print(\"Shape of X_new_with_sequences:\", X_new_with_sequences.shape)\n",
        "predicted_returns_scaled = model.predict(X_new_with_sequences)\n",
        "print(\"Shape of predicted_returns_scaled:\", predicted_returns_scaled.shape)\n",
        "y_reshaped = y_actual[-len(predicted_returns_scaled):].values.reshape(-1, 1)\n",
        "print(\"Shape of y_reshaped:\", y_reshaped.shape)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = np.mean((predicted_returns_scaled - y_reshaped) ** 2)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KJA4AMDc-RUu"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "Q1q3TBKJ86hp",
        "outputId": "6a7204f2-fd70-4397-dccb-eeab0fae841a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Close</th>\n",
              "      <th>Volume</th>\n",
              "      <th>Returns</th>\n",
              "      <th>Predicted_Returns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.069897</td>\n",
              "      <td>0.000788</td>\n",
              "      <td>0.021936</td>\n",
              "      <td>0.038471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.173595</td>\n",
              "      <td>0.003059</td>\n",
              "      <td>0.101059</td>\n",
              "      <td>0.016336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.018159</td>\n",
              "      <td>0.142623</td>\n",
              "      <td>0.010759</td>\n",
              "      <td>0.065919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.675796</td>\n",
              "      <td>0.003148</td>\n",
              "      <td>0.693151</td>\n",
              "      <td>-0.000751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.459421</td>\n",
              "      <td>0.248154</td>\n",
              "      <td>0.033716</td>\n",
              "      <td>-0.002462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3879</th>\n",
              "      <td>0.144611</td>\n",
              "      <td>0.039750</td>\n",
              "      <td>0.049465</td>\n",
              "      <td>0.016044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3880</th>\n",
              "      <td>0.065158</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.026536</td>\n",
              "      <td>0.031988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3881</th>\n",
              "      <td>0.336114</td>\n",
              "      <td>0.002029</td>\n",
              "      <td>0.190448</td>\n",
              "      <td>0.004390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3882</th>\n",
              "      <td>0.851249</td>\n",
              "      <td>0.004648</td>\n",
              "      <td>0.121916</td>\n",
              "      <td>0.000425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3883</th>\n",
              "      <td>0.224841</td>\n",
              "      <td>0.028097</td>\n",
              "      <td>0.012742</td>\n",
              "      <td>0.018683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3884 rows  4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Close    Volume   Returns  Predicted_Returns\n",
              "0     0.069897  0.000788  0.021936           0.038471\n",
              "1     0.173595  0.003059  0.101059           0.016336\n",
              "2     0.018159  0.142623  0.010759           0.065919\n",
              "3     0.675796  0.003148  0.693151          -0.000751\n",
              "4     0.459421  0.248154  0.033716          -0.002462\n",
              "...        ...       ...       ...                ...\n",
              "3879  0.144611  0.039750  0.049465           0.016044\n",
              "3880  0.065158  0.000548  0.026536           0.031988\n",
              "3881  0.336114  0.002029  0.190448           0.004390\n",
              "3882  0.851249  0.004648  0.121916           0.000425\n",
              "3883  0.224841  0.028097  0.012742           0.018683\n",
              "\n",
              "[3884 rows x 4 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Reshape predicted_returns_scaled to have shape (3884, 1)\n",
        "predicted_returns_reshaped = predicted_returns_scaled.reshape(-1, 1)\n",
        "\n",
        "# Remove the first timesteps rows from new_data_scaled to match the length of predicted_returns_scaled\n",
        "new_data_scaled_trimmed = new_data_scaled[timesteps-1:]\n",
        "\n",
        "# Create a new scaler object for the predicted returns with feature_range=(0, 1)\n",
        "returns_scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Fit the returns_scaler on the predicted returns\n",
        "returns_scaler.fit(predicted_returns_reshaped)\n",
        "\n",
        "# Transform the predicted returns with the fitted scaler\n",
        "predicted_returns_unscaled = returns_scaler.inverse_transform(predicted_returns_reshaped)\n",
        "\n",
        "# Horizontally stack predicted_returns_unscaled with new_data_scaled_trimmed\n",
        "combined_data = np.hstack((new_data_scaled_trimmed, predicted_returns_unscaled))\n",
        "\n",
        "column_names = ['Close', 'Volume', 'Returns', 'Predicted_Returns']\n",
        "\n",
        "# Convert the combined_data array to a DataFrame\n",
        "combined_df = pd.DataFrame(combined_data, columns=column_names)\n",
        "combined_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "id": "UR3KcRFOBnk4",
        "outputId": "abf9eff3-795c-41de-a9b4-344b0b24730e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\amitk\\AppData\\Local\\Temp\\ipykernel_19632\\2350246641.py:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  new_data_df['Predicted_Returns'] = combined_data[:, -1]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Date</th>\n",
              "      <th>ticker</th>\n",
              "      <th>Close</th>\n",
              "      <th>volume</th>\n",
              "      <th>Returns</th>\n",
              "      <th>Predicted_Returns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3191</th>\n",
              "      <td>3191</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>AMZN</td>\n",
              "      <td>161.485992</td>\n",
              "      <td>87798000.0</td>\n",
              "      <td>3.355245</td>\n",
              "      <td>0.038471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>359</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>UAL</td>\n",
              "      <td>46.580002</td>\n",
              "      <td>9204500.0</td>\n",
              "      <td>-0.711554</td>\n",
              "      <td>0.016336</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>HLT</td>\n",
              "      <td>147.796417</td>\n",
              "      <td>2652300.0</td>\n",
              "      <td>2.172959</td>\n",
              "      <td>0.065919</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3545</th>\n",
              "      <td>3545</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>WMT</td>\n",
              "      <td>141.254273</td>\n",
              "      <td>7577200.0</td>\n",
              "      <td>-0.044265</td>\n",
              "      <td>-0.000751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1421</th>\n",
              "      <td>1421</td>\n",
              "      <td>1/10/2022</td>\n",
              "      <td>MAR</td>\n",
              "      <td>160.183914</td>\n",
              "      <td>2228400.0</td>\n",
              "      <td>0.134011</td>\n",
              "      <td>-0.002462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2295</th>\n",
              "      <td>2295</td>\n",
              "      <td>9/8/2022</td>\n",
              "      <td>RCL</td>\n",
              "      <td>45.160000</td>\n",
              "      <td>6233700.0</td>\n",
              "      <td>-0.652134</td>\n",
              "      <td>0.016044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>879</th>\n",
              "      <td>879</td>\n",
              "      <td>9/8/2022</td>\n",
              "      <td>CCL</td>\n",
              "      <td>10.080000</td>\n",
              "      <td>41765200.0</td>\n",
              "      <td>-0.776794</td>\n",
              "      <td>0.031988</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1587</th>\n",
              "      <td>1587</td>\n",
              "      <td>9/8/2022</td>\n",
              "      <td>MAR</td>\n",
              "      <td>158.139710</td>\n",
              "      <td>1690800.0</td>\n",
              "      <td>14.688463</td>\n",
              "      <td>0.004390</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>526</th>\n",
              "      <td>526</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>UAL</td>\n",
              "      <td>39.259998</td>\n",
              "      <td>7405000.0</td>\n",
              "      <td>-0.751739</td>\n",
              "      <td>0.000425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172</th>\n",
              "      <td>172</td>\n",
              "      <td>9/9/2022</td>\n",
              "      <td>HLT</td>\n",
              "      <td>135.472473</td>\n",
              "      <td>1993200.0</td>\n",
              "      <td>2.450649</td>\n",
              "      <td>0.018683</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3884 rows  7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Unnamed: 0       Date ticker       Close      volume    Returns  \\\n",
              "3191        3191  1/10/2022   AMZN  161.485992  87798000.0   3.355245   \n",
              "359          359  1/10/2022    UAL   46.580002   9204500.0  -0.711554   \n",
              "5              5  1/10/2022    HLT  147.796417   2652300.0   2.172959   \n",
              "3545        3545  1/10/2022    WMT  141.254273   7577200.0  -0.044265   \n",
              "1421        1421  1/10/2022    MAR  160.183914   2228400.0   0.134011   \n",
              "...          ...        ...    ...         ...         ...        ...   \n",
              "2295        2295   9/8/2022    RCL   45.160000   6233700.0  -0.652134   \n",
              "879          879   9/8/2022    CCL   10.080000  41765200.0  -0.776794   \n",
              "1587        1587   9/8/2022    MAR  158.139710   1690800.0  14.688463   \n",
              "526          526   9/9/2022    UAL   39.259998   7405000.0  -0.751739   \n",
              "172          172   9/9/2022    HLT  135.472473   1993200.0   2.450649   \n",
              "\n",
              "      Predicted_Returns  \n",
              "3191           0.038471  \n",
              "359            0.016336  \n",
              "5              0.065919  \n",
              "3545          -0.000751  \n",
              "1421          -0.002462  \n",
              "...                 ...  \n",
              "2295           0.016044  \n",
              "879            0.031988  \n",
              "1587           0.004390  \n",
              "526            0.000425  \n",
              "172            0.018683  \n",
              "\n",
              "[3884 rows x 7 columns]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Drop extra rows from the DataFrame to match the length of 'combined_data'\n",
        "new_data_df = new_data_df.iloc[:len(combined_data)]\n",
        "\n",
        "# Add the unscaled predicted returns as a new column 'Predicted_Returns' to the DataFrame\n",
        "new_data_df['Predicted_Returns'] = combined_data[:, -1]\n",
        "\n",
        "new_data_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8UvJbgPgE93"
      },
      "outputs": [],
      "source": [
        "# #joblib.dump(model, 'LSTM_model.pkl')\n",
        "# model.save('LSTM_model.h5')\n",
        "# model.save('my_model.keras')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_mf0H8Q6c1F",
        "outputId": "2656e266-b714-4c61-8dca-c761d7da88ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "122/122 [==============================] - 1s 4ms/step\n",
            "Mean Squared Error: 8.30605330651183\n"
          ]
        }
      ],
      "source": [
        "# declare variable for actual values\n",
        "y_actual = new_data_df[\"Returns\"]\n",
        "\n",
        "# Make predictions using trained LSTM model\n",
        "predicted_returns_scaled = model.predict(X_new_with_sequences)\n",
        "\n",
        "# Reshape y_actual to match the shape of predicted_returns_scaled\n",
        "y_reshaped = y_actual[-len(predicted_returns_scaled):].values.reshape(-1, 1)\n",
        "\n",
        "# Calculate Mean Squared Error (MSE)\n",
        "mse = np.mean((predicted_returns_unscaled - y_reshaped) ** 2)\n",
        "print(f\"Mean Squared Error: {mse}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "KxPzraVG7fSz",
        "outputId": "885b254e-56b5-4418-9255-6bf4ba357d3a"
      },
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'Predicted_Returns'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "File \u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
            "File \u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Predicted_Returns'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the predicted returns and actual returns from the DataFrame\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m predicted_returns \u001b[38;5;241m=\u001b[39m \u001b[43mnew_data_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredicted_Returns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      6\u001b[0m actual_returns \u001b[38;5;241m=\u001b[39m new_data_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturns\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Create a scatter plot\u001b[39;00m\n",
            "File \u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
            "File \u001b[1;32m~\\anaconda3.1\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
            "\u001b[1;31mKeyError\u001b[0m: 'Predicted_Returns'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# Extract the predicted returns and actual returns from the DataFrame\n",
        "predicted_returns = new_data_df[\"Predicted_Returns\"]\n",
        "actual_returns = new_data_df[\"Returns\"]\n",
        "\n",
        "# Create a scatter plot\n",
        "plt.scatter(predicted_returns, actual_returns, alpha=0.5)\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel(\"Predicted Returns\")\n",
        "plt.ylabel(\"Actual Returns\")\n",
        "plt.title(\"Predicted Returns vs. Actual Returns (Scatter Plot)\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W7ouqouuCbM8"
      },
      "outputs": [],
      "source": [
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "# Load and preprocess your training data\n",
        "# scaler.fit(training_data)\n",
        "\n",
        "# Get user input for price and volume\n",
        "price = float(input(\"Enter the price: \"))\n",
        "volume = float(input(\"Enter the volume: \"))\n",
        "\n",
        "# Preprocess the user input\n",
        "input_data = np.array([[price, volume]])  # Shape: (1, 2)\n",
        "preprocessed_input = preprocess_input_data(input_data, scaler)  # Shape: (1, 10, 2)\n",
        "\n",
        "# Make predictions using the preprocessed input data\n",
        "prediction = loaded_LSTM_model.predict_on_batch(preprocessed_input)\n",
        "\n",
        "# Display the prediction to the user\n",
        "print(\"Predicted Return:\", prediction)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the LSTM model\n",
        "def create_lstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(100, input_shape=input_shape))\n",
        "    model.add(Dense(100, activation='relu'))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "# Loop through unique tickers\n",
        "unique_tickers = df['ticker'].unique()\n",
        "results = {}  # Dictionary to store MSE results\n",
        "predicted_dfs = {}  # Dictionary to store DataFrames with predicted results\n",
        "# Create a dictionary to hold models and scalers\n",
        "ticker_models = {}  # Dictionary to store trained models\n",
        "scalers = {}  # Dictionary to store associated scalers\n",
        "for ticker in unique_tickers:\n",
        "    print(f\"Processing ticker: {ticker}\")\n",
        "\n",
        "    # Filter data for the current ticker\n",
        "    ticker_data = df[df['ticker'] == ticker]\n",
        "\n",
        "    # Extract price, volume, and returns columns\n",
        "    input_data = ticker_data[['Close', 'volume']].values\n",
        "    target_returns = ticker_data['Returns'].values\n",
        "    print(\"Input Data Shape:\", input_data.shape)\n",
        "    print(\"Target Returns Shape:\", target_returns.shape)\n",
        "\n",
        "    # Define a MinMaxScaler and fit it to your data\n",
        "    scaler = MinMaxScaler()\n",
        "    scaler.fit(input_data)  # Fit the scaler to your input data\n",
        "\n",
        "    # Preprocess input data\n",
        "    preprocessed_input = preprocess_input_data(input_data, scaler)\n",
        "\n",
        "    # Create LSTM sequences\n",
        "    timesteps = 10\n",
        "    X_lstm = create_lstm_sequences(preprocessed_input, timesteps)\n",
        "    y_lstm = target_returns[timesteps - 1:]\n",
        "\n",
        "    # Split sequences into train and test sets\n",
        "    train_size = int(len(X_lstm) * 0.8)\n",
        "    X_train, X_test = X_lstm[:train_size], X_lstm[train_size:]\n",
        "    y_train, y_test = y_lstm[:train_size], y_lstm[train_size:]\n",
        "\n",
        "    # Create the LSTM model\n",
        "    lstm_model = create_lstm_model(X_lstm.shape[1:])\n",
        "\n",
        "    print(\"LSTM Model Summary:\")\n",
        "    lstm_model.summary()\n",
        "\n",
        "    # Train the model using the current ticker's training data\n",
        "    lstm_model.fit(X_train, y_train, epochs=60, batch_size=32, verbose=1)\n",
        "    print(\"Model training completed.\")\n",
        "\n",
        "    #Save the trained model and scaler in the dictionaries\n",
        "    ticker_models[ticker] = lstm_model\n",
        "    scalers[ticker] = scaler\n",
        "\n",
        "    # Evaluate the model for the ticker\n",
        "    mse = lstm_model.evaluate(X_test, y_test)\n",
        "    results[ticker] = mse\n",
        "\n",
        "    predicted_returns_scaled = lstm_model.predict(X_lstm)\n",
        "\n",
        "    # Inverse transform scaled predicted returns to get the actual return values\n",
        "    predicted_returns_actual = scaler.inverse_transform(np.concatenate((predicted_returns_scaled, np.zeros_like(predicted_returns_scaled)), axis=1))\n",
        "\n",
        "    # Flatten the 2D array to convert it into a 1D array\n",
        "    predicted_returns_actual_flatten = predicted_returns_actual.flatten()\n",
        "\n",
        "    # Adjust the length of Close and volume arrays to match\n",
        "    ticker_data_length = len(ticker_data)\n",
        "    adjusted_timesteps = ticker_data_length - timesteps + 1\n",
        "    close_values = ticker_data['Close'].values[-adjusted_timesteps:]\n",
        "    volume_values = ticker_data['volume'].values[-adjusted_timesteps:]\n",
        "\n",
        "    # Adjust the length of the flattened array to match the length of other arrays\n",
        "    predicted_returns_actual_reshaped = predicted_returns_actual_flatten[:len(target_returns) - (timesteps - 1)]\n",
        "\n",
        "    # create common length variable\n",
        "    common_length = min(len(ticker_data['Close'].values), len(ticker_data['volume'].values), len(target_returns) - (timesteps - 1))\n",
        "\n",
        "    # Create a new DataFrame for the current ticker\n",
        "    new_df = pd.DataFrame({\n",
        "        'Close': ticker_data['Close'].values[timesteps - 1:][:common_length],  # Adjust for timesteps used in sequences\n",
        "        'volume': ticker_data['volume'].values[timesteps - 1:][:common_length],  # Adjust for timesteps used in sequences\n",
        "        'Actual_Returns': target_returns[timesteps - 1:][:common_length],\n",
        "        'Predicted_Returns': predicted_returns_actual_reshaped[:common_length]\n",
        "    })\n",
        "\n",
        "    # Store the new DataFrame in the dictionary using the ticker as the key\n",
        "    predicted_dfs[ticker] = new_df\n",
        "\n",
        "    print(f\"Mean Squared Error for {ticker}: {mse}\")\n",
        "\n",
        "# Print the results for each ticker\n",
        "for ticker, mse in results.items():\n",
        "    print(f\"Mean Squared Error for {ticker}: {mse}\")\n",
        "\n",
        "# Access the predicted DataFrames using the predicted_dfs dictionary\n",
        "# for ticker, df in predicted_dfs.items():\n",
        "#     print(\"Predicted DataFrame for\", ticker, \":\\n\", df)\n",
        "\n",
        "# Save the models and scalers using Keras and pickle\n",
        "for ticker, model in ticker_models.items():\n",
        "    model.save(f'model_{ticker}.h5')\n",
        "\n",
        "for ticker, scaler in scalers.items():\n",
        "    with open(f'scaler_{ticker}.pkl', 'wb') as f:\n",
        "        pickle.dump(scaler, f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sA3S-wai9cOC",
        "outputId": "b8af2698-c7d8-4552-8620-4ee24d33e211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ticker: AMZN\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_122\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_122 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_244 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_245 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 8ms/step - loss: 6.8105\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 6.1868\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 6.0607\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 6.0584\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.8345\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.5703\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1766\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.8763\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.8905\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.9139\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.9258\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7731\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.8464\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7385\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7256\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7225\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7489\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.8120\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.7085\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.7449\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.6937\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 4.7116\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 4.7002\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 4.6757\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 4.6476\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7296\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.6551\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.6492\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.6840\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.6421\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.6235\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5883\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.6884\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.6189\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.5946\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5718\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.5414\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5275\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5641\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5769\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5623\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.6121\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.6260\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5556\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.4846\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.4868\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5095\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5214\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5263\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.5595\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5237\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.5374\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.4766\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.4620\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.4373\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.4527\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.4394\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.5203\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.4653\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 4.4203\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 9.3242\n",
            "40/40 [==============================] - 0s 2ms/step\n",
            "Mean Squared Error for AMZN: 9.324239730834961\n",
            "Processing ticker: WMT\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_123\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_123 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_246 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_247 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 7ms/step - loss: 5.9396\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.6477\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.6288\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.5894\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.5633\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.5083\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.4970\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.4014\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.3051\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2813\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2333\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2052\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.2247\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2019\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.2071\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.2033\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.1790\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.1629\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 5.1609\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 5.2730\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 5.1619\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 5.1991\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 5.2282\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 5.1452\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1444\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1370\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1159\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1226\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1128\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1194\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1072\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.1042\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.0966\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 5.1028\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.0708\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 5.0529\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 5.0673\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 5.0542\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 5.0048\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 4.9924\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.9430\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.9278\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.8603\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.8413\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 4.8618\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 4.7494\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 4.9069\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.7538\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 4.8356\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.6909\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.7448\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.7225\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.7350\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 4.5978\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 6ms/step - loss: 4.5329\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 4.5335\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 4.5099\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 4.6670\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 4.5761\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 4.5509\n",
            "Model training completed.\n",
            "8/8 [==============================] - 1s 6ms/step - loss: 4.6654\n",
            "40/40 [==============================] - 0s 2ms/step\n",
            "Mean Squared Error for WMT: 4.665365219116211\n",
            "Processing ticker: CCL\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_124\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_124 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_248 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_249 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 4s 14ms/step - loss: 0.4112\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3956\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3737\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3489\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3375\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3330\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3306\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3315\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3298\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3287\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3250\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 16ms/step - loss: 0.3261\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.3255\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.3258\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3259\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3241\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3231\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3231\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3223\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3227\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3223\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3233\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3214\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3206\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 16ms/step - loss: 0.3205\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 0.3201\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3214\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3202\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3182\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3205\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3194\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3170\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3217\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3185\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 1s 20ms/step - loss: 0.3190\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3179\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3191\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3180\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3175\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3182\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3187\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3175\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3176\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3177\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3160\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3165\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3170\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3197\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3196\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3155\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3143\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3156\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 0.3154\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 0.3151\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3156\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3157\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3150\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3140\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3146\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3170\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3728\n",
            "40/40 [==============================] - 1s 3ms/step\n",
            "Mean Squared Error for CCL: 0.3727980852127075\n",
            "Processing ticker: HLT\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_125\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_125 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_250 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_251 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 9ms/step - loss: 3.0144\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.8339\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.8348\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.8232\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.8249\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.8167\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.8029\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 2.7927\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 2.7907\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 2.7606\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 1s 18ms/step - loss: 2.7241\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2.7052\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6943\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6794\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6594\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6581\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.6982\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.7032\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 2.6725\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6581\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6561\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6746\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6598\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6537\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6510\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6495\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6511\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.6475\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6578\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6667\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.6449\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6535\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6504\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6538\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6619\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 2.6427\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 1s 17ms/step - loss: 2.6493\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6622\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6474\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6515\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6476\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6486\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6479\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6455\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6618\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 2.6429\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 2.6454\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.6398\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6407\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 2.6519\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6515\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 2.6418\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6470\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6507\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6346\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 2.6410\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6325\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6427\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 2.6394\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 2.6312\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 1.9870\n",
            "40/40 [==============================] - 1s 4ms/step\n",
            "Mean Squared Error for HLT: 1.9870258569717407\n",
            "Processing ticker: RCL\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_126\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_126 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_252 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_253 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 10ms/step - loss: 1.7648\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5840\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5966\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5882\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 1.5816\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 1s 19ms/step - loss: 1.5954\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 1.5924\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.5830\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5783\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5848\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5969\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 1s 16ms/step - loss: 1.5903\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 1s 15ms/step - loss: 1.5810\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.5782\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5824\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5772\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5762\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5806\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5725\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5752\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 1.5732\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.5738\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5671\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5822\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5774\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5683\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5649\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5638\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5605\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5644\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5611\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5633\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5613\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5594\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5601\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5700\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5567\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.5535\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5569\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5562\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 1.5546\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5606\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5590\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 1.5533\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 15ms/step - loss: 1.5528\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5503\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5640\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5532\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 1.5486\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5494\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5488\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5448\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5506\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5426\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 1.5475\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5463\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5466\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5414\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5414\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 1.5445\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 2.1249\n",
            "40/40 [==============================] - 0s 4ms/step\n",
            "Mean Squared Error for RCL: 2.124875545501709\n",
            "Processing ticker: MAR\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_127\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_127 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_254 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_255 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 9ms/step - loss: 4.3850\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 4.0119\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 4.0137\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 4.0174\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 4.0294\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 3.9925\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 4.0045\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.9856\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9904\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.9970\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 3.9800\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9841\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.0048\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.0060\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 4.0386\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.9725\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9848\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9844\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9609\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9620\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9559\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9602\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.9480\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.9184\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.8863\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.8563\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8505\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.8622\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.8425\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8361\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.8225\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8350\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8097\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.8299\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8082\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8004\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8025\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8453\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8319\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 3.7990\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 3.8040\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 3.7885\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 3.7868\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7808\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7753\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.8083\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7821\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7944\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7676\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7674\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.7785\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7829\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7968\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7833\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7646\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7530\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7661\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7575\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 3.7420\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 3.7548\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 5ms/step - loss: 3.3947\n",
            "40/40 [==============================] - 0s 4ms/step\n",
            "Mean Squared Error for MAR: 3.3946592807769775\n",
            "Processing ticker: CAKE\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_128\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_128 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_256 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_257 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 9ms/step - loss: 0.4112\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4086\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4134\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4085\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4091\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4093\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4087\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4059\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4083\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4063\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4055\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4062\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.4042\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4029\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4014\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4033\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3994\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3967\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3968\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3953\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3966\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3972\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3932\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3944\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3944\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3931\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3924\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3923\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3925\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3925\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3936\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3915\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3896\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3900\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3907\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3900\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3898\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3889\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3903\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3875\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3873\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.3866\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.3876\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3878\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3861\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3867\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.3871\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3867\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3848\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3864\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3818\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3832\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3829\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3809\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.3804\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3792\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3782\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3790\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.3759\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.3760\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 3ms/step - loss: 0.3681\n",
            "40/40 [==============================] - 0s 3ms/step\n",
            "Mean Squared Error for CAKE: 0.36809787154197693\n",
            "Processing ticker: UAL\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_129\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_129 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_258 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_259 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 3s 9ms/step - loss: 0.9692\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.9608\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9531\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.9454\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.9460\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9356\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9274\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.9267\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.9097\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.9048\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.9026\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.8888\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8847\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8907\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8871\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8783\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8829\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.8806\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.8837\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8816\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8705\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8693\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8699\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8623\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8683\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8619\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8710\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8567\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8425\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8866\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8846\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.8583\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8526\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8367\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.8231\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7893\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8192\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8305\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.8210\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7731\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.7662\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7669\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7147\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.8250\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7353\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7043\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7226\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7001\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7020\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6987\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7719\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7212\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.7100\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7114\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7477\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.7134\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.7039\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6906\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6897\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.6869\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.8028\n",
            "40/40 [==============================] - 0s 3ms/step\n",
            "Mean Squared Error for UAL: 0.8028367161750793\n",
            "Processing ticker: DAL\n",
            "Input Data Shape: (1259, 2)\n",
            "Target Returns Shape: (1259,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_130\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_130 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_260 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_261 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 9ms/step - loss: 0.4177\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4123\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4156\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4157\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4126\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4114\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4125\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4143\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4118\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4110\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4113\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4128\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 1s 33ms/step - loss: 0.4112\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 14ms/step - loss: 0.4105\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 1s 21ms/step - loss: 0.4116\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4119\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4110\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.4101\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4102\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4098\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4090\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4136\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4097\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4103\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4117\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4091\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4103\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4097\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4124\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4093\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4090\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4092\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4094\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4087\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4087\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4104\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.4087\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4094\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4084\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4093\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4086\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4090\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4081\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4083\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4087\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4109\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4084\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.4086\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4082\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 12ms/step - loss: 0.4096\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4069\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4073\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4066\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4062\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4067\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.4072\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4061\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4075\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4058\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.4051\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.3696\n",
            "40/40 [==============================] - 0s 3ms/step\n",
            "Mean Squared Error for DAL: 0.36963358521461487\n",
            "Processing ticker: RUTH\n",
            "Input Data Shape: (1258, 2)\n",
            "Target Returns Shape: (1258,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_131\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_131 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_262 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_263 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "32/32 [==============================] - 2s 8ms/step - loss: 0.1060\n",
            "Epoch 2/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0378\n",
            "Epoch 3/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0366\n",
            "Epoch 4/60\n",
            "32/32 [==============================] - 0s 7ms/step - loss: 0.0368\n",
            "Epoch 5/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0355\n",
            "Epoch 6/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0357\n",
            "Epoch 7/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0359\n",
            "Epoch 8/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0347\n",
            "Epoch 9/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0348\n",
            "Epoch 10/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0345\n",
            "Epoch 11/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0340\n",
            "Epoch 12/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0339\n",
            "Epoch 13/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0341\n",
            "Epoch 14/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0381\n",
            "Epoch 15/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0343\n",
            "Epoch 16/60\n",
            "32/32 [==============================] - 0s 11ms/step - loss: 0.0335\n",
            "Epoch 17/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0336\n",
            "Epoch 18/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0338\n",
            "Epoch 19/60\n",
            "32/32 [==============================] - 0s 13ms/step - loss: 0.0343\n",
            "Epoch 20/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0339\n",
            "Epoch 21/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0336\n",
            "Epoch 22/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0337\n",
            "Epoch 23/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0337\n",
            "Epoch 24/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0334\n",
            "Epoch 25/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0329\n",
            "Epoch 26/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0328\n",
            "Epoch 27/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0327\n",
            "Epoch 28/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0339\n",
            "Epoch 29/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0353\n",
            "Epoch 30/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0330\n",
            "Epoch 31/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0345\n",
            "Epoch 32/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0325\n",
            "Epoch 33/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0330\n",
            "Epoch 34/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0326\n",
            "Epoch 35/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0323\n",
            "Epoch 36/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0323\n",
            "Epoch 37/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0325\n",
            "Epoch 38/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0322\n",
            "Epoch 39/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0321\n",
            "Epoch 40/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0330\n",
            "Epoch 41/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0330\n",
            "Epoch 42/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0331\n",
            "Epoch 43/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0328\n",
            "Epoch 44/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0327\n",
            "Epoch 45/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0321\n",
            "Epoch 46/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0321\n",
            "Epoch 47/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0328\n",
            "Epoch 48/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0322\n",
            "Epoch 49/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0321\n",
            "Epoch 50/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0322\n",
            "Epoch 51/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0322\n",
            "Epoch 52/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0323\n",
            "Epoch 53/60\n",
            "32/32 [==============================] - 0s 8ms/step - loss: 0.0330\n",
            "Epoch 54/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0337\n",
            "Epoch 55/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0330\n",
            "Epoch 56/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0322\n",
            "Epoch 57/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0321\n",
            "Epoch 58/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0324\n",
            "Epoch 59/60\n",
            "32/32 [==============================] - 0s 9ms/step - loss: 0.0325\n",
            "Epoch 60/60\n",
            "32/32 [==============================] - 0s 10ms/step - loss: 0.0321\n",
            "Model training completed.\n",
            "8/8 [==============================] - 0s 4ms/step - loss: 0.0297\n",
            "40/40 [==============================] - 0s 2ms/step\n",
            "Mean Squared Error for RUTH: 0.029683917760849\n",
            "Processing ticker: WH\n",
            "Input Data Shape: (912, 2)\n",
            "Target Returns Shape: (912,)\n",
            "LSTM Model Summary:\n",
            "Model: \"sequential_132\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_132 (LSTM)             (None, 100)               41200     \n",
            "                                                                 \n",
            " dense_264 (Dense)           (None, 100)               10100     \n",
            "                                                                 \n",
            " dense_265 (Dense)           (None, 1)                 101       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 51,401\n",
            "Trainable params: 51,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/60\n",
            "23/23 [==============================] - 2s 7ms/step - loss: 0.8126\n",
            "Epoch 2/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8095\n",
            "Epoch 3/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8078\n",
            "Epoch 4/60\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8044\n",
            "Epoch 5/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.8040\n",
            "Epoch 6/60\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.8003\n",
            "Epoch 7/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7970\n",
            "Epoch 8/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7927\n",
            "Epoch 9/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7858\n",
            "Epoch 10/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7689\n",
            "Epoch 11/60\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7737\n",
            "Epoch 12/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7588\n",
            "Epoch 13/60\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7571\n",
            "Epoch 14/60\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7525\n",
            "Epoch 15/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7536\n",
            "Epoch 16/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7528\n",
            "Epoch 17/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7487\n",
            "Epoch 18/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7472\n",
            "Epoch 19/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7497\n",
            "Epoch 20/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7441\n",
            "Epoch 21/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7454\n",
            "Epoch 22/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7388\n",
            "Epoch 23/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7427\n",
            "Epoch 24/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7416\n",
            "Epoch 25/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7387\n",
            "Epoch 26/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7388\n",
            "Epoch 27/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7397\n",
            "Epoch 28/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7387\n",
            "Epoch 29/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7405\n",
            "Epoch 30/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7430\n",
            "Epoch 31/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7367\n",
            "Epoch 32/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7402\n",
            "Epoch 33/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7383\n",
            "Epoch 34/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7366\n",
            "Epoch 35/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7375\n",
            "Epoch 36/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7340\n",
            "Epoch 37/60\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7349\n",
            "Epoch 38/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7367\n",
            "Epoch 39/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7330\n",
            "Epoch 40/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7372\n",
            "Epoch 41/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7385\n",
            "Epoch 42/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7326\n",
            "Epoch 43/60\n",
            "23/23 [==============================] - 0s 11ms/step - loss: 0.7321\n",
            "Epoch 44/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7298\n",
            "Epoch 45/60\n",
            "23/23 [==============================] - 0s 12ms/step - loss: 0.7315\n",
            "Epoch 46/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7266\n",
            "Epoch 47/60\n",
            "23/23 [==============================] - 0s 10ms/step - loss: 0.7268\n",
            "Epoch 48/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7272\n",
            "Epoch 49/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7267\n",
            "Epoch 50/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7225\n",
            "Epoch 51/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7165\n",
            "Epoch 52/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7217\n",
            "Epoch 53/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7172\n",
            "Epoch 54/60\n",
            "23/23 [==============================] - 0s 7ms/step - loss: 0.7159\n",
            "Epoch 55/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7164\n",
            "Epoch 56/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7117\n",
            "Epoch 57/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7133\n",
            "Epoch 58/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7127\n",
            "Epoch 59/60\n",
            "23/23 [==============================] - 0s 9ms/step - loss: 0.7168\n",
            "Epoch 60/60\n",
            "23/23 [==============================] - 0s 8ms/step - loss: 0.7012\n",
            "Model training completed.\n",
            "6/6 [==============================] - 0s 4ms/step - loss: 1.4815\n",
            "29/29 [==============================] - 0s 3ms/step\n",
            "Mean Squared Error for WH: 1.4815267324447632\n",
            "Mean Squared Error for AMZN: 9.324239730834961\n",
            "Mean Squared Error for WMT: 4.665365219116211\n",
            "Mean Squared Error for CCL: 0.3727980852127075\n",
            "Mean Squared Error for HLT: 1.9870258569717407\n",
            "Mean Squared Error for RCL: 2.124875545501709\n",
            "Mean Squared Error for MAR: 3.3946592807769775\n",
            "Mean Squared Error for CAKE: 0.36809787154197693\n",
            "Mean Squared Error for UAL: 0.8028367161750793\n",
            "Mean Squared Error for DAL: 0.36963358521461487\n",
            "Mean Squared Error for RUTH: 0.029683917760849\n",
            "Mean Squared Error for WH: 1.4815267324447632\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}